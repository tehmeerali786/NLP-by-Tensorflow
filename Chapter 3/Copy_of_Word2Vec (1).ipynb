{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsg_RXCuPyzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are all the models we'll be using later. Make sure you can import them \n",
        "# before procedding further\n",
        "%matplotlib inline\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import bz2\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "import nltk # standard preprocessing\n",
        "import operator # sorting items in dictionary by value \n",
        "# nltk.download() #tokensizers/plunkt/PY3/english.pickle\n",
        "from math import ceil\n",
        "import csv\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-txXMtZ6QpkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = '/content/wikipedia2text-extracted.txt.bz2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RqDoIf94cfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /wikipedia2text-extracted.txt.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xSAFpj05NRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/wikipedia2text-extracted.txt.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vwrUJU06MB9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3878146e-40a8-42c7-957d-e4da3c424ce1"
      },
      "source": [
        " nltk.download('punkt')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPKsisFeQ9lN",
        "colab_type": "code",
        "outputId": "85a8ddee-630a-463b-d914-52c7eb710d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def read_data(filename):\n",
        "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
        "\n",
        "  with bz2.BZ2File(filename) as f:\n",
        "    data = []\n",
        "    file_string = f.read().decode('utf-8')\n",
        "    file_string = nltk.word_tokenize(file_string)\n",
        "    data.extend(file_string)\n",
        "  return data\n",
        "  \n",
        "words = read_data(filename)\n",
        "print('Data size %d' % len(words))\n",
        "print('Example words (start): ',words[:10])\n",
        "print('Example words (end): ',words[-10:])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data size 11631723\n",
            "Example words (start):  ['Propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
            "Example words (end):  ['useless', 'for', 'cultivation', '.', 'and', 'people', 'have', 'sex', 'there', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXvZf70CzSIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ3tP7nAREi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/sample_data/california_housing_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWPnHxLjR5sH",
        "colab_type": "code",
        "outputId": "71c06f5b-de75-47a4-ae91-bc252e3bb60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>housing_median_age</th>\n",
              "      <th>total_rooms</th>\n",
              "      <th>total_bedrooms</th>\n",
              "      <th>population</th>\n",
              "      <th>households</th>\n",
              "      <th>median_income</th>\n",
              "      <th>median_house_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-122.05</td>\n",
              "      <td>37.37</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3885.0</td>\n",
              "      <td>661.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>606.0</td>\n",
              "      <td>6.6085</td>\n",
              "      <td>344700.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-118.30</td>\n",
              "      <td>34.26</td>\n",
              "      <td>43.0</td>\n",
              "      <td>1510.0</td>\n",
              "      <td>310.0</td>\n",
              "      <td>809.0</td>\n",
              "      <td>277.0</td>\n",
              "      <td>3.5990</td>\n",
              "      <td>176500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-117.81</td>\n",
              "      <td>33.78</td>\n",
              "      <td>27.0</td>\n",
              "      <td>3589.0</td>\n",
              "      <td>507.0</td>\n",
              "      <td>1484.0</td>\n",
              "      <td>495.0</td>\n",
              "      <td>5.7934</td>\n",
              "      <td>270500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-118.36</td>\n",
              "      <td>33.82</td>\n",
              "      <td>28.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6.1359</td>\n",
              "      <td>330000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-119.67</td>\n",
              "      <td>36.33</td>\n",
              "      <td>19.0</td>\n",
              "      <td>1241.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>850.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>2.9375</td>\n",
              "      <td>81700.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   longitude  latitude  ...  median_income  median_house_value\n",
              "0    -122.05     37.37  ...         6.6085            344700.0\n",
              "1    -118.30     34.26  ...         3.5990            176500.0\n",
              "2    -117.81     33.78  ...         5.7934            270500.0\n",
              "3    -118.36     33.82  ...         6.1359            330000.0\n",
              "4    -119.67     36.33  ...         2.9375             81700.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQOAlwnUR7JX",
        "colab_type": "code",
        "outputId": "efb1aa13-ba17-4506-a803-1649b9e4b014",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "print('Data size %d'% len(words))\n",
        "print('Example words (start): ', words[:10])\n",
        "print('Example words (end)', words[-10:])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-bdc9f128cf99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data size %d'\u001b[0m\u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Example words (start): '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Example words (end)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_PfFFkJSUXm",
        "colab_type": "code",
        "outputId": "7f3feefd-2a1e-4eb9-caef-c7b45c2c11ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqrs3344VYAP",
        "colab_type": "text"
      },
      "source": [
        "## Read Data with Preprocessing with NLTK\n",
        "\n",
        "Reads data as it is to a string, convert to lower-case and tokenize it using the nltk library. This code reads data in 1MB portions as processing the full text at once slows down the task and returns a list of words. You will have to download the necessary tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWpMVUpYS1xF",
        "colab_type": "code",
        "outputId": "6323a6b3-60a7-4a0d-cef0-41883f24f436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def read_data(filename):\n",
        "          \"\"\"\n",
        "              Extract the first file enclosed in a zip file as a list of words\n",
        "              and pre-processes it using the nltk python library\n",
        "          \"\"\"\n",
        "\n",
        "          with bz2.BZ2File(filename) as f:\n",
        "\n",
        "            data = []\n",
        "            file_size = os.stat(filename).st_size\n",
        "            chunck_size = 1024 * 1024 # reading 1mb at a time as dataset is modaretaly large\n",
        "            print('Data data ......')\n",
        "            for i in range(ceil(file_size//chunck_size)+1):\n",
        "              bytes_to_read = min(chunck_size, file_size - (i * chunck_size))\n",
        "              file_string = f.read(bytes_to_read).decode('utf-8')\n",
        "              file_string = file_string.lower()\n",
        "              # tokenizes a string to words residing a list\n",
        "              file_string = nltk.word_tokenize(file_string)\n",
        "              data.extend(file_string)\n",
        "\n",
        "            return data\n",
        "\n",
        "words = read_data(filename)\n",
        "print('Data size %d' % len(words))\n",
        "print('Example words (start): ', words[:10])\n",
        "print('Example words (end): ', words[-10:])\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data data ......\n",
            "Data size 3360286\n",
            "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
            "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go6JX7kETqzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hnng2DZecl_",
        "colab_type": "text"
      },
      "source": [
        "## Building the Dictionaries\n",
        "\n",
        "Builds the following. To understand each of theses elements, let us also assume the text, \"I like to go school\"\n",
        "\n",
        "* `dictionary` : maps a string word to an ID (e.g. {I:0, like:1, to:2, go:3, school 4})\n",
        "* `reverse_dictionary`: maps an ID to a string word (e.g. {0:I, 1:like, 2:to, 3:go, 4:school}\n",
        "* `count`: List of list of (word, frequency) elements (e.g. [(I,1),(like,1),(to,2),(go,1),(school,1)]\n",
        "* `data` : Contain the string of text we read, where string words are replaced with word IDs (e.g. [0, 1, 2, 3, 2, 4])\n",
        "\n",
        "It also introduces an additional special token `UNK` to denote rare words to are too rare to make use of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAFTFWj-flM-",
        "colab_type": "code",
        "outputId": "30a630fe-5cee-4a9b-e66a-5118e90f4f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# We restrict our vocabulary size to 50000\n",
        "vocabulary_size = 50000\n",
        "\n",
        "def build_dataset(words):\n",
        "  count = [['UNK', -1]]\n",
        "  # Get only the vocabulary_size most common words as the vocabulary\n",
        "  # All the other words will be replaced with UNK token\n",
        "\n",
        "  count.extend(collections.Counter(words).most_common(vocabulary_size -1))\n",
        "  dictionary = dict()\n",
        "\n",
        "  # Create an ID for each word by giving the current length of the dictionary\n",
        "  # And adding that item to the dictionary \n",
        "  for word, _ in count:\n",
        "    dictionary[word] = len(dictionary)\n",
        "\n",
        "  data = list()\n",
        "  unk_count = 0\n",
        "  # Traverse through all the text we have and produce a list\n",
        "  # Where each element correponds to the ID of the word found at that index\n",
        "\n",
        "  for word in words:\n",
        "    # If word is in dictionary, use the word ID\n",
        "    # Else the id of the special token \"UNK\"\n",
        "    if word in dictionary:\n",
        "      index = dictionary[word]\n",
        "    else:\n",
        "      index = 0 # dictionary['UNK']\n",
        "      unk_count = unk_count + 1\n",
        "    data.append(index)\n",
        "\n",
        "    # Update the count variable with the UNK resources\n",
        "    count[0][1] = unk_count\n",
        "\n",
        "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
        "    # Make sure the dictionary is of the size of vocabulary\n",
        "    assert len(dictionary) == vocabulary_size\n",
        "\n",
        "    return data, count, dictionary, reverse_dictionary\n",
        "\n",
        "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
        "print('Most common words (+UNK)', count[:5])\n",
        "print('Sample data', data[:10])\n",
        "del words \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Most common words (+UNK) [['UNK', 0], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323)]\n",
            "Sample data [1721]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZSrzequopRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1j41__8wy4S",
        "colab_type": "text"
      },
      "source": [
        "## Generating Batches of Data for Skim Gram\n",
        "\n",
        "Generates a batch or target words (batch) and a batch of corresponding context words (labels). It reads 2*window_size+1 words at a time (called a span) and create 2*window_size data points in a single points. The function continue in this manner until batch_size datapoints are created. Everytime we reach the end of the word sequance, we start from beginning. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ri3Q5MfYyMEI",
        "colab_type": "code",
        "outputId": "e4b3f9e4-16fa-46ab-fc08-d2ac7c1adaa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "data_index = 0\n",
        "\n",
        "def generate_batch_skip_gram(batch_size, window_size):\n",
        "  # data_index is updated by 1 everytime we read a data point\n",
        "  global data_index\n",
        "\n",
        "  # Two numpy array to hold target words (batch)\n",
        "  # and context words (labels)\n",
        "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
        "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "\n",
        "  # span defines the total window size, where\n",
        "  # data we consider at an instance looks as follow.\n",
        "  # [ skip_window target skip_window]\n",
        "  span = 2 * window_size + 1\n",
        "\n",
        "  # The buffer holds the data contained within the span\n",
        "  buffer = collections.deque(maxlen=span)\n",
        "\n",
        "  # Fill the buffer and update the data_index \n",
        "  for _ in range(span):\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "\n",
        "  # This is the number of context words we sample for a single target word\n",
        "  num_samples = 2*window_size\n",
        "\n",
        "  # We break the batch reading into two for loops\n",
        "  # The inner for loop fills in the batch and labels with\n",
        "  # num_samples data points using data contained within span\n",
        "  # The outer for loop repeat this for batch_size//num_samples times\n",
        "  # to produce a full batch\n",
        "\n",
        "  for i in range(batch_size//num_samples):\n",
        "    k=0\n",
        "    # avoid the target word itself as a prediction\n",
        "    # fill in batch and label numpy array\n",
        "    for j in list(range(window_size)) + list(range(window_size + 1, 2 * window_size + 1)):\n",
        "      batch[i * num_samples + k] = buffer[window_size]\n",
        "      labels[i * num_samples + k, 0] = buffer[j]\n",
        "      k += 1\n",
        "\n",
        "    # Everytime we read num_samples data points.\n",
        "    # We have created the maximum number of datapoints possible\n",
        "    # within a single span, so we need to move the span by 1\n",
        "    # to create a fresh new span\n",
        "    buffer.append(data[data_index])\n",
        "    data_index = (data_index + 1) % len(data)\n",
        "  return batch, labels\n",
        "\n",
        "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
        "\n",
        "for window_size in [1, 2]:\n",
        "  data_index = 0\n",
        "  batch, labels = generate_batch_skip_gram(batch_size=8, window_size=window_size)\n",
        "  print('\\n with window_size = %d:' % window_size)\n",
        "  print(' batch:', [reverse_dictionary[bi] for bi in batch])\n",
        "  print(' labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data: ['propaganda']\n",
            "\n",
            " with window_size = 1:\n",
            " batch: ['propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda']\n",
            " labels: ['propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda']\n",
            "\n",
            " with window_size = 2:\n",
            " batch: ['propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda']\n",
            " labels: ['propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda', 'propaganda']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i28iEhGE_sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QnLbuVZOlkm",
        "colab_type": "text"
      },
      "source": [
        "## Skip-Gram Algorithm\n",
        "### Defining Hyperparameters\n",
        "\n",
        "Here we define several hyperparameters including batch_size (amount of samples in a single batch) embedding_size (size of embedding vectors) window_size (context window size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5Wnwz1uOswY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128 # Data points in a single batch\n",
        "embedding_size = 128 # Dimension of the embedding vector\n",
        "window_size = 4 # How many words to consider left and right.\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors \n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 50\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as \n",
        "# some moderately rare words as well\n",
        "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_examples = np.append(valid_examples,random.sample(range(1000, 1000+valid_window), valid_size),axis=0)\n",
        "\n",
        "num_sampled = 32 # Number of negative examples to sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrqxLLvYQ-P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEFgGRNYUMCA",
        "colab_type": "text"
      },
      "source": [
        "## Defining Inputs and Outputs\n",
        "\n",
        "Here we define placeholders for feeding in training inputs and outputs (each of size batch_size) and a constant tensor to contain validation examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEoJyJVCUPW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Training input data (target word IDs)\n",
        "train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "\n",
        "# Training input label data (context word IDS)\n",
        "train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "# Validation input data, we don't need a placeholder\n",
        "# as we have already defined the IDs of the word selected\n",
        "# as validation data\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydRode-YVg1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOXrKQN0worw",
        "colab_type": "text"
      },
      "source": [
        "# Defining Model Parameters and Other Variables\n",
        "\n",
        "We now define several TensorFlow variables such as an embedding layer (embeddings) and neural network parameters (softmax_weights and softmax_biases)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKkIhgCkwr21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Variables \n",
        "\n",
        "# Embedding Layer, contains the word embeddings\n",
        "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "\n",
        "# Softmax Weights and Biases \n",
        "softmax_weights = tf.Variable(\n",
        "    tf.truncated_normal([vocabulary_size, embedding_size], stddev=0.5 / math.sqrt(embedding_size))\n",
        ")\n",
        "\n",
        "softmax_biases = tf.Variable(tf.random_uniform([vocabulary_size], 0.0, 0.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubm813df7F80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEUl_4H17Wip",
        "colab_type": "text"
      },
      "source": [
        "#Defining the Model Computations\n",
        "\n",
        "We first defing a lookup function to fetch the corresponding embedding vectors for a set of given inputs. With that, we define negative sampling loss function tf.nn.sampled_softmax_loss which takes in the embedding vectors and previously defined neural network parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42OcCmSI7Yqx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model\n",
        "# Lookup embeddings for a batch of inputs\n",
        "embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
        "\n",
        "# Compute the softmax loss, using a sample of negative labels each time.\n",
        "loss = tf.reduce_mean(\n",
        "    tf.nn.sampled_softmax_loss(\n",
        "        weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
        "        labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eaJ4IjB8Oer",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wURssx7w8Yt7",
        "colab_type": "text"
      },
      "source": [
        "## Calculating Word Similarities\n",
        "\n",
        "We calculate the similarity between two given words in terms of the cosine distance. To do this efficiently we use matrix operations to do so, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGns0vm18cMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the similarity between minibatch examples and all embeddings\n",
        "# We use the cosine distance:\n",
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "normalized_embeddings = embeddings / norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(\n",
        "normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdq1bMrv9G-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKk9BXoR9-bd",
        "colab_type": "text"
      },
      "source": [
        "## Model Parameter Optimizer\n",
        "\n",
        "We then define a constant learning rate and an optimizer which uses the Adagrad method. Feel free to experiment with other optimizers listed here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYT7OmXN-Au8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7b25bb5d-ed2a-44ef-ba6f-4acea0f3e9ec"
      },
      "source": [
        "# Optimizer.\n",
        "optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w3HWDNu-Fsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi7FPP39_SmI",
        "colab_type": "text"
      },
      "source": [
        "#Running the Skip-Gram Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqcV3vdh_U8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c318681-223b-4aa8-dd52-919c4462ff38"
      },
      "source": [
        "num_steps = 100001\n",
        "skip_losses = []\n",
        "\n",
        "# ConfigProto is a way of providing various configuration settings\n",
        "# required to execute the graph\n",
        "\n",
        "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as session:\n",
        "  # Initialize the variables in the graph\n",
        "  tf.global_variables_initializer().run()\n",
        "  print('Initialized')\n",
        "  average_loss = 0\n",
        "\n",
        "  # Train the Word2Vec model for num_step iterations\n",
        "  for step in range(num_steps):\n",
        "\n",
        "    # Generate a single batch of data\n",
        "    batch_data, batch_labels = generate_batch_skip_gram(\n",
        "        batch_size, window_size\n",
        "    )\n",
        "\n",
        "    # Populate the feed_dict  and run the optimizer (minimize loss)\n",
        "    # and compute the loss\n",
        "    feed_dict = {train_dataset: batch_data, train_labels: batch_labels}\n",
        "    _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
        "\n",
        "    # Update the average loss variable\n",
        "    average_loss += 1\n",
        "\n",
        "    if (step+1) % 2000==0:\n",
        "      if step > 0:\n",
        "        average_loss = average_loss / 2000\n",
        "\n",
        "      skip_losses.append(average_loss)\n",
        "      # The average loss is an estimate of the loss over the last 2000 batches\n",
        "      print('Average loss at step %d: %f' % (step+1, average_loss))\n",
        "      average_loss = 0\n",
        "\n",
        "    # Evaluating validation set word similarities\n",
        "    if (step+1) % 10000 == 0:\n",
        "      sim = similarity.eval()\n",
        "      # Here we compute the top_k closest words for a given validation word\n",
        "      # in term of the cosine distance\n",
        "      # We do this for all the words in the validation set\n",
        "      # Note: This is an expensive step\n",
        "      for i in range(valid_size):\n",
        "        valid_word = reverse_dictionary[valid_examples[i]]\n",
        "        top_k = 8 # number of nearest neighbors \n",
        "        nearest = (-sim[i, :]).argsort()[1: top_k+1]\n",
        "        log = 'Nearest to %s' % valid_word\n",
        "        for k in range(top_k):\n",
        "          close_word = reverse_dictionary[nearest[k]]\n",
        "          log = '%s %s,' % (log, close_word)\n",
        "        print(log)\n",
        "      skip_gram_final_embeddings = normalized_embeddings.eval()\n",
        "\n",
        "  # We will save the word vectors learned and the loss over time\n",
        "  # as this information is required later for comparisons\n",
        "  np.save('skip_embeddings', skip_gram_final_embeddings)\n",
        "\n",
        "  with open('skip_losses.csv', 'wt') as f:\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerow(skip_losses)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Average loss at step 2000: 1.000000\n",
            "Average loss at step 4000: 1.000000\n",
            "Average loss at step 6000: 1.000000\n",
            "Average loss at step 8000: 1.000000\n",
            "Average loss at step 10000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 12000: 1.000000\n",
            "Average loss at step 14000: 1.000000\n",
            "Average loss at step 16000: 1.000000\n",
            "Average loss at step 18000: 1.000000\n",
            "Average loss at step 20000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 22000: 1.000000\n",
            "Average loss at step 24000: 1.000000\n",
            "Average loss at step 26000: 1.000000\n",
            "Average loss at step 28000: 1.000000\n",
            "Average loss at step 30000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 32000: 1.000000\n",
            "Average loss at step 34000: 1.000000\n",
            "Average loss at step 36000: 1.000000\n",
            "Average loss at step 38000: 1.000000\n",
            "Average loss at step 40000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 42000: 1.000000\n",
            "Average loss at step 44000: 1.000000\n",
            "Average loss at step 46000: 1.000000\n",
            "Average loss at step 48000: 1.000000\n",
            "Average loss at step 50000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 52000: 1.000000\n",
            "Average loss at step 54000: 1.000000\n",
            "Average loss at step 56000: 1.000000\n",
            "Average loss at step 58000: 1.000000\n",
            "Average loss at step 60000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 62000: 1.000000\n",
            "Average loss at step 64000: 1.000000\n",
            "Average loss at step 66000: 1.000000\n",
            "Average loss at step 68000: 1.000000\n",
            "Average loss at step 70000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 72000: 1.000000\n",
            "Average loss at step 74000: 1.000000\n",
            "Average loss at step 76000: 1.000000\n",
            "Average loss at step 78000: 1.000000\n",
            "Average loss at step 80000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 82000: 1.000000\n",
            "Average loss at step 84000: 1.000000\n",
            "Average loss at step 86000: 1.000000\n",
            "Average loss at step 88000: 1.000000\n",
            "Average loss at step 90000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n",
            "Average loss at step 92000: 1.000000\n",
            "Average loss at step 94000: 1.000000\n",
            "Average loss at step 96000: 1.000000\n",
            "Average loss at step 98000: 1.000000\n",
            "Average loss at step 100000: 1.000000\n",
            "Nearest to ( know, grange, crucially, estates-general, slings, threats, anchorages, arithmetic,\n",
            "Nearest to city bernicia, fervently, obsolescent, experiential, ting, coded, decisively, braithwaite,\n",
            "Nearest to it dimetrodon, cansez, chauvet, unelected, handicapped, feduccia, joy, pink,\n",
            "Nearest to of scandinavian, marpat, painless, mcclory, subsonic, affirms, non-official, 219,\n",
            "Nearest to its classmate, evert, pointer, cliffs, sähkö, lynching, benfica, husserl,\n",
            "Nearest to one seconds, 5pm, role-playing, ics, sauvignon, 1516, hamlin, 334,\n",
            "Nearest to a wrecks, 2004, slaveholders, scorned, rijn, equus, éirinn, 3300,\n",
            "Nearest to which defiled, depart, linden, dates, alzheimer, complexion, limbo, 7.5,\n",
            "Nearest to first vagueness, descending, reconciling, ship, nilus, background, spray, logarithms,\n",
            "Nearest to this forecasts, roadway, watt, esslin, resolve, nucleosynthesis, staphylococcus, charlatan,\n",
            "Nearest to have infertility, superboy, wishes, favoritism, sling, trans-neptunian, sikorski, identities,\n",
            "Nearest to their 2004., consultants, uprisings, éparses, billboard, offer, kitase, 2-3,\n",
            "Nearest to and licences, brandenburg, 6:3, ukemi, milton, ethanol-water, huxley, grudging,\n",
            "Nearest to for jubilee, commemorative, disciples, flood-retaining, microphone, saffron, quadrille, adjunct,\n",
            "Nearest to in dogfight, redcoats, coronel, aprons, preslav, radicalized, spacing, hermaphrodite,\n",
            "Nearest to 's pathé, liquidation, awal, positioned, drifters, suffered, zeta, estates-general,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjNwFOEEZjre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YefVbUYObbmi",
        "colab_type": "text"
      },
      "source": [
        "#Visulizing the Learnings of the Skip-Gram Algorithm\n",
        "\n",
        "## Finding Only the Words Clustered Together Instead of Sparsely Distributed Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2aAInnSbjGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_clustered_embeddings(embeddings, distance_threshold, sample_threshold):\n",
        "  \"\"\"\n",
        "      Find only the closely clustered embeddings.\n",
        "      This gets rid of more sparsly distributed word embeddings and make the visualization clearer\n",
        "      This is useful for t-SNE visualization\n",
        "\n",
        "      distance_threshold: maximum distance between two points to qualify as neighbors\n",
        "      sample_threshold: number of neighbors required to be considered cluster\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # calculate cosine similarity\n",
        "  cosine_sim = np.dot(embeddings, np.transpose(embeddings))\n",
        "  norm = np.dot(np.sum(embeddings**2, axis=1).reshape(-1, 1), np.sum(np.transpose(embeddings)**2, axis=0).reshape(1, -1))\n",
        "  assert cosine_sim.shape == norm.shape\n",
        "  cosine_sim /= norm\n",
        "\n",
        "  # make all the diagnol entries zero otherwise this will be picked as highest\n",
        "  np.fill_diagonal(cosine_sim, -1.0)\n",
        "\n",
        "  argmax_cos_sim = np.argmax(cosine_sim, axis=1)\n",
        "  mod_cos_sim = cosine_sim\n",
        "\n",
        "  # Find the maximums in a loop to count if there are more than n items above threshold\n",
        "  for _ in range(sample_threshold - 1):\n",
        "    argmax_cos_sim = np.argmax(cosine_sim, axis=1)\n",
        "    mod_cos_sim[np.arange(mod_cos_sim.shape[0]), argmax_cos_sim] = -1\n",
        "  \n",
        "  max_cosine_sim = np.max(mod_cos_sim, axis=1)\n",
        "\n",
        "  return np.where(max_cosine_sim > distance_threshold)[0]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvCHlWCYfmmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhfMpcYUfzZg",
        "colab_type": "text"
      },
      "source": [
        "## Computing the t-SNE Visualization of Word Embeddings Using Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iMNTbqjf0iP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9ce0041c-3045-4b73-a851-ba336469f887"
      },
      "source": [
        "num_points = 1000 # We will use a large sample space to build the T-SNE manifold then prune it using cosine similarity\n",
        "\n",
        "tsne = TSNE(perplexity = 30, n_components=2, init='pca', n_iter=5000)\n",
        "\n",
        "print('Fitting embeddings to T-SNE. This can take some time .......')\n",
        "# get the T-SNE manifold\n",
        "selected_embeddings = skip_gram_final_embeddings[:num_points, :]\n",
        "two_d_embeddings = tsne.fit_transform(selected_embeddings)\n",
        "\n",
        "print('Pruning the T-SNE embeddings')\n",
        "# Prune the embeddings by getting ones only more than n-many sample above the similarity threshold\n",
        "# this ultraclustters the visualization\n",
        "selected_ids = find_clustered_embeddings(selected_embeddings, .25, 10)\n",
        "two_d_embeddings = two_d_embeddings[selected_ids, :]\n",
        "\n",
        "print('Out of ', num_points, ' samples ', selected_ids.shape[0], ' samples were selected by pruning ')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting embeddings to T-SNE. This can take some time .......\n",
            "Pruning the T-SNE embeddings\n",
            "Out of  1000  samples  0  samples were selected by pruning \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hs86RLfjZpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsItDrFzjuXi",
        "colab_type": "text"
      },
      "source": [
        "## Plotting the t-SNE Results with Matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzh63cBijwio",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "f9341641-8d43-4351-db0c-21eee1eea5dc"
      },
      "source": [
        "def plot(embeddings, labels):\n",
        "\n",
        "  n_clusters = 20 # number of clusters\n",
        "  # automatically build a discreate set of colors, each of cluster\n",
        "  cmap = pylab.get_cmap(\"Spectral\")\n",
        "  label_colors = [cmap(float(i) /n_clusters) for i in range(n_clusters)]\n",
        "\n",
        "  assert embeddings.shape[0] >= len(labels), 'More label than embeddings'\n",
        "\n",
        "  # Define K-Means \n",
        "  kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=0).fit(embeddings)\n",
        "  kmeans_labels = kmeans.label_\n",
        "\n",
        "\n",
        "  pylabe.figure(figsize=(15, 15)) # in inches\n",
        "\n",
        "  # Plot all the embeddings and their corresponding words\n",
        "  for i, (label, klabel) in enumerate(zip(labels, kmeans_labels)):\n",
        "    x, y = embeddings[i, :]\n",
        "    pylab.scatter(x, y, c=label_colors[klabel])\n",
        "\n",
        "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
        "                   ha='right', va='bottom', fontsize=10)\n",
        "    \n",
        "  # Use for saving the figure if needed\n",
        "  # pylab.savefig('word_embeddings.png')\n",
        "  pylab.show()\n",
        "\n",
        "words = [reverse_dictionary[i] for i in selected_ids]\n",
        "plot(two_d_embeddings, words)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-8bf177dcd607>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreverse_dictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwo_d_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-52-8bf177dcd607>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# Define K-Means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k-means++'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mkmeans_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"C\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_x\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=[np.float64, np.float32],\n\u001b[0;32m--> 859\u001b[0;31m                         order=order, copy=self.copy_x)\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;31m# verify that the number of samples given is larger than k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    584\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 586\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 2)) while a minimum of 1 is required."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxJs-7VYnAgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}